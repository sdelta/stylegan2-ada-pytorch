{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTasHr2HNERXYX/zgVIDVm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sdelta/ImageGen/blob/main/stylegan2_clip_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare dependencies"
      ],
      "metadata": {
        "id": "pQao9vNHVWOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "i18v-FoPKu0P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e266e599-b75b-41f4-eac5-b9668d97bdd5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install click requests tqdm pyspng ninja imageio-ffmpeg==0.4.3 open_clip_torch"
      ],
      "metadata": {
        "id": "bbsLKrW85ISu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "663ae7ca-89e9-4e37-a664-d6a1dc9583a1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (2.25.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
            "Collecting pyspng\n",
            "  Downloading pyspng-0.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.9/205.9 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 KB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting imageio-ffmpeg==0.4.3\n",
            "  Downloading imageio_ffmpeg-0.4.3-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting open_clip_torch\n",
            "  Downloading open_clip_torch-2.9.3-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pyspng) (1.21.6)\n",
            "Collecting protobuf==3.20.*\n",
            "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from open_clip_torch) (0.14.0+cu116)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from open_clip_torch) (1.13.0+cu116)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from open_clip_torch) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.9.0->open_clip_torch) (4.4.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->open_clip_torch) (0.2.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->open_clip_torch) (3.8.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->open_clip_torch) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->open_clip_torch) (21.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->open_clip_torch) (7.1.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub->open_clip_torch) (3.0.9)\n",
            "Installing collected packages: sentencepiece, ninja, pyspng, protobuf, imageio-ffmpeg, ftfy, huggingface-hub, open_clip_torch\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ftfy-6.1.1 huggingface-hub-0.11.1 imageio-ffmpeg-0.4.3 ninja-1.11.1 open_clip_torch-2.9.3 protobuf-3.20.3 pyspng-0.1.1 sentencepiece-0.1.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/ffhq-res256-mirror-paper256-noaug.pkl"
      ],
      "metadata": {
        "id": "qYdu-X-YkRIU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e68c3abb-0101-4dbd-fcbd-066873213f7a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-10 04:34:45--  https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/ffhq-res256-mirror-paper256-noaug.pkl\n",
            "Resolving nvlabs-fi-cdn.nvidia.com (nvlabs-fi-cdn.nvidia.com)... 13.249.85.31, 13.249.85.23, 13.249.85.118, ...\n",
            "Connecting to nvlabs-fi-cdn.nvidia.com (nvlabs-fi-cdn.nvidia.com)|13.249.85.31|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 295744285 (282M) [binary/octet-stream]\n",
            "Saving to: ‘ffhq-res256-mirror-paper256-noaug.pkl’\n",
            "\n",
            "ffhq-res256-mirror- 100%[===================>] 282.04M  29.0MB/s    in 10s     \n",
            "\n",
            "2023-01-10 04:34:56 (28.3 MB/s) - ‘ffhq-res256-mirror-paper256-noaug.pkl’ saved [295744285/295744285]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/datasets/ffhq_256/ffhq.zip ./"
      ],
      "metadata": {
        "id": "_mj9MfXDkUhD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "DFtl-xc6qQ5e",
        "outputId": "ca8856dc-3df8-40f1-803c-e91daa11616d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  ffhq-res256-mirror-paper256-noaug.pkl  ffhq.zip\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Debug code"
      ],
      "metadata": {
        "id": "5H1lulrZVHrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "\n",
        "image = Image.open(requests.get(url, stream=True).raw)"
      ],
      "metadata": {
        "id": "vfMs5KkzZVc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import open_clip\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "assert device == \"cuda\"\n",
        "\n",
        "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32-quickgelu', pretrained='laion400m_e32')\n",
        "tokenizer = open_clip.get_tokenizer('ViT-B-32-quickgelu')\n",
        "\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "suL_JKk1e2KV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_images = [image]\n",
        "src_texts = [\"cat\", \"dog\"]\n",
        "images = torch.tensor(np.stack([preprocess(img) for img in src_images])).to(device)\n",
        "texts = tokenizer(src_texts).to(device)"
      ],
      "metadata": {
        "id": "lMBVag3Fe2FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_features = model.encode_text(texts)\n",
        "texts_features /= texts_features.norm(dim=-1, keepdim=True)"
      ],
      "metadata": {
        "id": "Bm-vCLCjfm5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_features = model.encode_image(images)\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)"
      ],
      "metadata": {
        "id": "iosyOcQXfmwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim = torch.matmul(texts_features, image_features.permute(1, 0))"
      ],
      "metadata": {
        "id": "xNhpKJtRe1_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images.shape"
      ],
      "metadata": {
        "id": "pzG2WLOMlccM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim"
      ],
      "metadata": {
        "id": "v1R6_cqDfYvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import functional as tfn\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "def normalize(x, mean, std):\n",
        "    mean = mean.unsqueeze(1).unsqueeze(2)\n",
        "    std = std.unsqueeze(1).unsqueeze(2)\n",
        "    return (x - mean) / std\n",
        "\n",
        "start = transforms.ToTensor()(image).unsqueeze(0).to(device)\n",
        "sized = tfn.interpolate(start, size=224, mode='bicubic')\n",
        "normed = normalize(\n",
        "    sized,\n",
        "    torch.tensor(open_clip.OPENAI_DATASET_MEAN).to(device),\n",
        "    torch.tensor(open_clip.OPENAI_DATASET_STD).to(device)\n",
        ")"
      ],
      "metadata": {
        "id": "h1Ie_4EzkcO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.equal(images, normed)"
      ],
      "metadata": {
        "id": "K5zCyHSAkcM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(images[0].cpu().permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "He3tOkPrkcKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(normed[0].cpu().permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "rJH6a-tZkcIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_lst = [\n",
        "    transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "    transforms.CenterCrop(224)\n",
        "]\n",
        "\n",
        "my_preprocess = transforms.Compose(tr_lst)"
      ],
      "metadata": {
        "id": "mtJB4HFtqd31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(tr_lst[0](start[0]).cpu().permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "GhsI6ko8q0Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_features.shape"
      ],
      "metadata": {
        "id": "738zIOvZrWZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = start.detach()\n",
        "input.requires_grad_(True)"
      ],
      "metadata": {
        "id": "v0itJEEc450S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPSubloss(object):\n",
        "    def __init__(self, device, clip_phrase):\n",
        "        self.device = device\n",
        "        self.model = model\n",
        "        self.model = self.model.to(device)\n",
        "        tokenizer = open_clip.get_tokenizer('ViT-B-32-quickgelu')\n",
        "        with torch.no_grad():\n",
        "            self.texts_features = self.model.encode_text(tokenizer([clip_phrase]).to(device))\n",
        "            self.texts_features /= self.texts_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    def _preprocess_images(self, images):\n",
        "        resized = torch.nn.functional.interpolate(images, size=224, mode='bicubic')\n",
        "        mean = torch.tensor(open_clip.OPENAI_DATASET_MEAN).to(self.device).unsqueeze(1).unsqueeze(2)\n",
        "        std = torch.tensor(open_clip.OPENAI_DATASET_STD).to(self.device).unsqueeze(1).unsqueeze(2)\n",
        "        return (resized - mean) / std\n",
        "        \n",
        "    def get_similarities(self, images):\n",
        "        images_features = self.model.encode_image(self._preprocess_images(images))\n",
        "        \n",
        "        images_norm = images_features.norm(dim=-1, keepdim=True) + 1e-5\n",
        "        print(images_norm.cpu())\n",
        "        #return (images_features / images_norm).permute(1, 0)\n",
        "        return torch.matmul(self.texts_features, (images_features / images_norm).permute(1, 0))\n",
        "\n",
        "clip_subloss = CLIPSubloss(device, \"glasses\")\n",
        "\n",
        "with torch.autograd.set_detect_anomaly(True):\n",
        "    gen_clip = clip_subloss.get_similarities(input)\n",
        "    gen_clip.mean().mul(4).backward()\n"
      ],
      "metadata": {
        "id": "mTjacEoF3foj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G7_xRn6E3fk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jYMktYKo3fgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SNzskHab3fb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mlfHd2IV3fXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y-_m7QL23fSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_TIkNmqsrWTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess"
      ],
      "metadata": {
        "id": "f_mJY1eykcFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cDnxIWbckcCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O8elj0ADkb-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jNyY0KAZfYnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load repo"
      ],
      "metadata": {
        "id": "w7R_gN5NVLWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -fR stylegan2-ada-pytorch"
      ],
      "metadata": {
        "id": "t7YNNLG_Y4sy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sdelta/stylegan2-ada-pytorch.git"
      ],
      "metadata": {
        "id": "8RyD4_beY4Hd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53c60802-c970-4aad-928f-aa3de0b9c339"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stylegan2-ada-pytorch'...\n",
            "remote: Enumerating objects: 241, done.\u001b[K\n",
            "remote: Counting objects: 100% (77/77), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 241 (delta 53), reused 1 (delta 0), pack-reused 164\u001b[K\n",
            "Receiving objects: 100% (241/241), 1.16 MiB | 8.97 MiB/s, done.\n",
            "Resolving deltas: 100% (136/136), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finetune"
      ],
      "metadata": {
        "id": "JNXc-ztnVONd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python stylegan2-ada-pytorch/train.py --outdir=drive/MyDrive/stylegan_finetuning --data=ffhq.zip \\\n",
        "    --mirror=1 --gpus=1 --resume=ffhq-res256-mirror-paper256-noaug.pkl --kimg=1500 --cfg=paper256 \\\n",
        "    --snap=10 --metrics=\"none\" \\\n",
        "    --freezed=10 --freezed_mapping=True \\\n",
        "    --clip_phrase='glasses' --clip_reg_interval=4"
      ],
      "metadata": {
        "id": "fqSSP0b06pqb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f56d7bfe-ae67-4f77-e75a-79a3699b6e5e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training options:\n",
            "{\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 10,\n",
            "  \"network_snapshot_ticks\": 10,\n",
            "  \"metrics\": [],\n",
            "  \"random_seed\": 0,\n",
            "  \"training_set_kwargs\": {\n",
            "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
            "    \"path\": \"ffhq.zip\",\n",
            "    \"use_labels\": false,\n",
            "    \"max_size\": 70000,\n",
            "    \"xflip\": true,\n",
            "    \"resolution\": 256\n",
            "  },\n",
            "  \"data_loader_kwargs\": {\n",
            "    \"pin_memory\": true,\n",
            "    \"num_workers\": 3,\n",
            "    \"prefetch_factor\": 2\n",
            "  },\n",
            "  \"G_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Generator\",\n",
            "    \"z_dim\": 512,\n",
            "    \"w_dim\": 512,\n",
            "    \"mapping_kwargs\": {\n",
            "      \"num_layers\": 8,\n",
            "      \"trainable\": false\n",
            "    },\n",
            "    \"synthesis_kwargs\": {\n",
            "      \"channel_base\": 16384,\n",
            "      \"channel_max\": 512,\n",
            "      \"num_fp16_res\": 4,\n",
            "      \"conv_clamp\": 256\n",
            "    }\n",
            "  },\n",
            "  \"D_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Discriminator\",\n",
            "    \"block_kwargs\": {\n",
            "      \"freeze_layers\": 10\n",
            "    },\n",
            "    \"mapping_kwargs\": {\n",
            "      \"trainable\": false\n",
            "    },\n",
            "    \"epilogue_kwargs\": {\n",
            "      \"mbstd_group_size\": 8\n",
            "    },\n",
            "    \"channel_base\": 16384,\n",
            "    \"channel_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"D_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"loss_kwargs\": {\n",
            "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
            "    \"r1_gamma\": 1,\n",
            "    \"clip_phrase\": \"glasses\"\n",
            "  },\n",
            "  \"clip_reg_interval\": 4,\n",
            "  \"total_kimg\": 1500,\n",
            "  \"batch_size\": 64,\n",
            "  \"batch_gpu\": 8,\n",
            "  \"ema_kimg\": 20,\n",
            "  \"ema_rampup\": null,\n",
            "  \"ada_target\": 0.6,\n",
            "  \"augment_kwargs\": {\n",
            "    \"class_name\": \"training.augment.AugmentPipe\",\n",
            "    \"xflip\": 1,\n",
            "    \"rotate90\": 1,\n",
            "    \"xint\": 1,\n",
            "    \"scale\": 1,\n",
            "    \"rotate\": 1,\n",
            "    \"aniso\": 1,\n",
            "    \"xfrac\": 1,\n",
            "    \"brightness\": 1,\n",
            "    \"contrast\": 1,\n",
            "    \"lumaflip\": 1,\n",
            "    \"hue\": 1,\n",
            "    \"saturation\": 1\n",
            "  },\n",
            "  \"resume_pkl\": \"ffhq-res256-mirror-paper256-noaug.pkl\",\n",
            "  \"ada_kimg\": 100,\n",
            "  \"run_dir\": \"drive/MyDrive/stylegan_finetuning/00043-ffhq-mirror-paper256-kimg1500-resumecustom-freezed10-freezed_mapping\"\n",
            "}\n",
            "\n",
            "Output directory:   drive/MyDrive/stylegan_finetuning/00043-ffhq-mirror-paper256-kimg1500-resumecustom-freezed10-freezed_mapping\n",
            "Training data:      ffhq.zip\n",
            "Training duration:  1500 kimg\n",
            "Number of GPUs:     1\n",
            "Number of images:   70000\n",
            "Image resolution:   256\n",
            "Conditional model:  False\n",
            "Dataset x-flips:    True\n",
            "\n",
            "Creating output directory...\n",
            "Launching processes...\n",
            "Loading training set...\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Num images:  140000\n",
            "Image shape: [3, 256, 256]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "Resuming from \"ffhq-res256-mirror-paper256-noaug.pkl\"\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
            "\n",
            "Generator             Parameters  Buffers  Output shape        Datatype\n",
            "---                   ---         ---      ---                 ---     \n",
            "mapping.fc0           -           262656   [8, 512]            float32 \n",
            "mapping.fc1           -           262656   [8, 512]            float32 \n",
            "mapping.fc2           -           262656   [8, 512]            float32 \n",
            "mapping.fc3           -           262656   [8, 512]            float32 \n",
            "mapping.fc4           -           262656   [8, 512]            float32 \n",
            "mapping.fc5           -           262656   [8, 512]            float32 \n",
            "mapping.fc6           -           262656   [8, 512]            float32 \n",
            "mapping.fc7           -           262656   [8, 512]            float32 \n",
            "mapping               -           512      [8, 14, 512]        float32 \n",
            "synthesis.b4.conv1    2622465     32       [8, 512, 4, 4]      float32 \n",
            "synthesis.b4.torgb    264195      -        [8, 3, 4, 4]        float32 \n",
            "synthesis.b4:0        8192        16       [8, 512, 4, 4]      float32 \n",
            "synthesis.b4:1        -           -        [8, 512, 4, 4]      float32 \n",
            "synthesis.b8.conv0    2622465     80       [8, 512, 8, 8]      float32 \n",
            "synthesis.b8.conv1    2622465     80       [8, 512, 8, 8]      float32 \n",
            "synthesis.b8.torgb    264195      -        [8, 3, 8, 8]        float32 \n",
            "synthesis.b8:0        -           16       [8, 512, 8, 8]      float32 \n",
            "synthesis.b8:1        -           -        [8, 512, 8, 8]      float32 \n",
            "synthesis.b16.conv0   2622465     272      [8, 512, 16, 16]    float32 \n",
            "synthesis.b16.conv1   2622465     272      [8, 512, 16, 16]    float32 \n",
            "synthesis.b16.torgb   264195      -        [8, 3, 16, 16]      float32 \n",
            "synthesis.b16:0       -           16       [8, 512, 16, 16]    float32 \n",
            "synthesis.b16:1       -           -        [8, 512, 16, 16]    float32 \n",
            "synthesis.b32.conv0   2622465     1040     [8, 512, 32, 32]    float16 \n",
            "synthesis.b32.conv1   2622465     1040     [8, 512, 32, 32]    float16 \n",
            "synthesis.b32.torgb   264195      -        [8, 3, 32, 32]      float16 \n",
            "synthesis.b32:0       -           16       [8, 512, 32, 32]    float16 \n",
            "synthesis.b32:1       -           -        [8, 512, 32, 32]    float32 \n",
            "synthesis.b64.conv0   1442561     4112     [8, 256, 64, 64]    float16 \n",
            "synthesis.b64.conv1   721409      4112     [8, 256, 64, 64]    float16 \n",
            "synthesis.b64.torgb   132099      -        [8, 3, 64, 64]      float16 \n",
            "synthesis.b64:0       -           16       [8, 256, 64, 64]    float16 \n",
            "synthesis.b64:1       -           -        [8, 256, 64, 64]    float32 \n",
            "synthesis.b128.conv0  426369      16400    [8, 128, 128, 128]  float16 \n",
            "synthesis.b128.conv1  213249      16400    [8, 128, 128, 128]  float16 \n",
            "synthesis.b128.torgb  66051       -        [8, 3, 128, 128]    float16 \n",
            "synthesis.b128:0      -           16       [8, 128, 128, 128]  float16 \n",
            "synthesis.b128:1      -           -        [8, 128, 128, 128]  float32 \n",
            "synthesis.b256.conv0  139457      65552    [8, 64, 256, 256]   float16 \n",
            "synthesis.b256.conv1  69761       65552    [8, 64, 256, 256]   float16 \n",
            "synthesis.b256.torgb  33027       -        [8, 3, 256, 256]    float16 \n",
            "synthesis.b256:0      -           16       [8, 64, 256, 256]   float16 \n",
            "synthesis.b256:1      -           -        [8, 64, 256, 256]   float32 \n",
            "---                   ---         ---      ---                 ---     \n",
            "Total                 22666210    2276816  -                   -       \n",
            "\n",
            "\n",
            "Discriminator  Parameters  Buffers  Output shape        Datatype\n",
            "---            ---         ---      ---                 ---     \n",
            "b256.fromrgb   -           272      [8, 64, 256, 256]   float16 \n",
            "b256.skip      -           8208     [8, 128, 128, 128]  float16 \n",
            "b256.conv0     -           36944    [8, 64, 256, 256]   float16 \n",
            "b256.conv1     -           73872    [8, 128, 128, 128]  float16 \n",
            "b256           -           16       [8, 128, 128, 128]  float16 \n",
            "b128.skip      -           32784    [8, 256, 64, 64]    float16 \n",
            "b128.conv0     -           147600   [8, 128, 128, 128]  float16 \n",
            "b128.conv1     -           295184   [8, 256, 64, 64]    float16 \n",
            "b128           -           16       [8, 256, 64, 64]    float16 \n",
            "b64.skip       -           131088   [8, 512, 32, 32]    float16 \n",
            "b64.conv0      -           590096   [8, 256, 64, 64]    float16 \n",
            "b64.conv1      -           1180176  [8, 512, 32, 32]    float16 \n",
            "b64            -           16       [8, 512, 32, 32]    float16 \n",
            "b32.skip       262144      16       [8, 512, 16, 16]    float16 \n",
            "b32.conv0      2359808     16       [8, 512, 32, 32]    float16 \n",
            "b32.conv1      2359808     16       [8, 512, 16, 16]    float16 \n",
            "b32            -           16       [8, 512, 16, 16]    float16 \n",
            "b16.skip       262144      16       [8, 512, 8, 8]      float32 \n",
            "b16.conv0      2359808     16       [8, 512, 16, 16]    float32 \n",
            "b16.conv1      2359808     16       [8, 512, 8, 8]      float32 \n",
            "b16            -           16       [8, 512, 8, 8]      float32 \n",
            "b8.skip        262144      16       [8, 512, 4, 4]      float32 \n",
            "b8.conv0       2359808     16       [8, 512, 8, 8]      float32 \n",
            "b8.conv1       2359808     16       [8, 512, 4, 4]      float32 \n",
            "b8             -           16       [8, 512, 4, 4]      float32 \n",
            "b4.mbstd       -           -        [8, 513, 4, 4]      float32 \n",
            "b4.conv        2364416     16       [8, 512, 4, 4]      float32 \n",
            "b4.fc          4194816     -        [8, 512]            float32 \n",
            "b4.out         513         -        [8, 1]              float32 \n",
            "---            ---         ---      ---                 ---     \n",
            "Total          21505025    2496480  -                   -       \n",
            "\n",
            "Setting up augmentation...\n",
            "Distributing across 1 GPUs...\n",
            "Setting up training phases...\n",
            "Exporting sample images...\n",
            "Initializing logs...\n",
            "Training for 1500 kimg...\n",
            "\n",
            "tick 0     kimg 0.1      time 2m 10s       sec/tick 19.0    sec/kimg 297.60  maintenance 110.8  cpumem 5.88   gpumem 8.79   augment 0.000 clip_loss -0.172\n",
            "tick 1     kimg 4.1      time 7m 19s       sec/tick 287.7   sec/kimg 71.35   maintenance 21.7   cpumem 4.99   gpumem 3.99   augment 0.031 clip_loss -0.184\n",
            "tick 2     kimg 8.1      time 12m 14s      sec/tick 295.1   sec/kimg 73.18   maintenance 0.0    cpumem 4.98   gpumem 4.00   augment 0.064 clip_loss -0.187\n",
            "tick 3     kimg 12.2     time 17m 10s      sec/tick 295.2   sec/kimg 73.22   maintenance 0.0    cpumem 4.94   gpumem 3.99   augment 0.090 clip_loss -0.195\n",
            "tick 4     kimg 16.2     time 22m 05s      sec/tick 295.1   sec/kimg 73.19   maintenance 0.0    cpumem 4.67   gpumem 4.01   augment 0.115 clip_loss -0.191\n",
            "tick 5     kimg 20.2     time 26m 55s      sec/tick 290.8   sec/kimg 72.13   maintenance 0.0    cpumem 4.51   gpumem 4.00   augment 0.141 clip_loss -0.195\n",
            "tick 6     kimg 24.3     time 31m 51s      sec/tick 295.4   sec/kimg 73.27   maintenance 0.0    cpumem 4.34   gpumem 4.00   augment 0.174 clip_loss -0.199\n",
            "tick 7     kimg 28.3     time 36m 47s      sec/tick 296.0   sec/kimg 73.42   maintenance 0.0    cpumem 4.19   gpumem 4.03   augment 0.205 clip_loss -0.204\n",
            "tick 8     kimg 32.3     time 41m 43s      sec/tick 296.0   sec/kimg 73.41   maintenance 0.0    cpumem 4.12   gpumem 4.01   augment 0.230 clip_loss -0.204\n",
            "tick 9     kimg 36.4     time 46m 34s      sec/tick 291.1   sec/kimg 72.20   maintenance 0.0    cpumem 4.08   gpumem 4.01   augment 0.246 clip_loss -0.207\n",
            "tick 10    kimg 40.4     time 51m 30s      sec/tick 296.1   sec/kimg 73.43   maintenance 0.0    cpumem 4.07   gpumem 4.01   augment 0.264 clip_loss -0.211\n",
            "tick 11    kimg 44.4     time 56m 48s      sec/tick 296.7   sec/kimg 73.58   maintenance 21.0   cpumem 5.16   gpumem 4.02   augment 0.289 clip_loss -0.216\n",
            "\n",
            "Aborted!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results\n",
        "\n",
        "Failure: not much more people with glasses from generator. Furthermore starting from 30 tick imagegrid show signs of mode collapse - all faces have similar structure\n",
        "\n",
        "But CLIP loss is steadily improving."
      ],
      "metadata": {
        "id": "RbzaptMJstgn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lMhvOglOqN0-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}